---
title: "Unit-Testing And Test-driven Development"
teaching: 45
exercises: 15
questions:
- "What is the benefit of unit-testing my code?"
- "How do I create and run unit tests?"
- "Why would I change my code after I got it to run?"
objectives:
- "Formalise documented examples as tests."
- "Use testthat functions to create and run tests."
keypoints:
- "Changing code is not always necessary, but often useful."
- "Tests provide a safety net for changing code."
- "Practice more TDD at [exercism.io](http://exercism.io/languages/)"
source: Rmd
---

```{r, include = FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-testthat-R-")
```


Because moving the test cases into their own files in a certain way
requires an expansion of the folder and file
structure of our R package, we are going to [`usethis`][usethis] again:

```{r use_test, eval=FALSE}
use_test("center")
```

A new file should be created and the console should show:

~~~
✔ Adding 'testthat' to Suggests field in DESCRIPTION
✔ Creating 'tests/testthat/'
✔ Writing 'tests/testthat.R'
✔ Writing 'tests/testthat/test-center.R'
● Modify 'test-center.R'
~~~
{: .source}

The file is pre-filled with a little example. Remove it, then cut our test case
out of `center.R` and paste it here.


To conclude this section about creating unit tests, let's again commit our results,
for example as "Span safety net for TDD".


### Test-driven development (TDD)

Remember that we updated `rescale()` with lower and upper bounds and default
values at the end of [the functions episode][ep-func]? We had to manually test that change with a
new example back then. We were repeating ourselves a bit more often than necessary
back then, weren't we? Of course, there are ways to automate the testing of code
changes, which gives you quick feedback whether your changes worked, or broke
anything.

To do that, we use `testthat`'s `auto_test_package()`
in the console, or in RStudio's `Build` pane the `More > Test Package` menu option, 
and notice the hopefully all green and `OK` `Results`.

With this safety net enabled, we will first update `test-center.R`, and then
update the code in `center.R` with the `desired = 0` default argument. This strategy of (re)writing (new)
tests before (re)writing the code-to-be-tested is called
"[test-driven design/development][TDD]". It is intended to reduce confirmation
bias when coding. If one already worked hard to get the code to run, one may be
_biased to_ let the tests _confirm_ what the code currently _does_, instead of
challenging it with tests about what it _should_ do. The latter is similar to the
scientific method of trying to disprove hypotheses, so that the truth remains.

[TDD]: https://en.wikipedia.org/wiki/Test-driven_development

> ## Update `test-center.R` with the argument default from [`center2.R`][ep-func].
>
> Think about which parts of the test code you need to change, to "break" the 
> existing implementation (with only the `desired` argument, but no `= 0` default)
> but to get it working again once the default is added to the function's code.
> 
> > ## Solution
> > ~~~
> > test_that("centering works with and without default arguments", {
> >   expect_equal(center(c(1, 2, 3) # `, 0` has to be deleted
> >                                        ), c(-1, 0, 1))
> >   expect_equal(center(c(1, 2, 3), 1), c(0, 1, 2))
> > })
> > ~~~
> > {: .r}
> {: .solution}
{: .challenge}

Save the file and if `auto_test_package()` is still running, you should see one
`Failed` `Result`. If you run the `expect_that("…", …)` or `expect_equal(…)`
block interactively, an `Error` should appear. This exactly what we want to see
before working on the code. Why?

> ## Update `center.R` with the argument default from [`center2.R`][ep-func].
>
> Don't copy-paste the code from earlier! Try instead to rely on the safety net
> and update the function code interactively, saving every now and then to
> trigger `auto_test_package()`. Don't forget to update the roxygen comments 
> and to `roxygenise()` again.
> 
> > ## Solution
> > 
> > See the ["Defining Defaults" section in the functions episode]( {{ site.root }} /03-func-R/#defining-defaults).
> > 
> {: .solution}
{: .challenge}


> ## Also update both `rescale` files with the argument defaults example from [`rescale2`][ep-func].
>
> You can either wrap this example in its own `test_that()` with a fitting
> description, or replace one of the existing tests. Either way, the test 
> explanation(s) should afterwards be checked whether they still reflect the 
> test code.
> 
> > ## Solution
> > ~~~
> > test_that("rescaling works with non-default arguments", {
> >   expect_equal(rescale(c(1, 2, 3), 1, 2), c(1.0, 1.5, 2.0))
> > })
> > ~~~
> > {: .r}
> {: .solution}
{: .challenge}

> ## Update `rescale.R` with a lower and upper bound argument and default values
>
> Don't copy-paste the code from earlier! Try instead to rely on the safety net
> and update the function code interactively, saving every now and then to
> trigger `auto_test_package()`.
> 
> > ## Solution
> > See the ["Defining Defaults" section in the functions episode]( {{ site.root }} /03-func-R/#a-function-with-default-argument-values).
> {: .solution}
{: .challenge}

While `auto_test_package()` is still running, play around with the code a bit more:

- change the placement of `(` parentheses `)`
- remove the `return()` statement to find out how R can implicitly return
  the last calculated value within a function
- shorten the code to 2 lines, or even only 1
- rename arguments to something more self-explanatory (basic "[refactoring]")

Some of those changes will result in errors, some will make the code more readable,
some will make it faster or slower.
To stop `auto_test_package()` in the console, press <kdb>Esc</kdb> or the red `STOP` button.
With automatic testing we can more quickly reach a sensible balance between investing
our time and energy into testing code improvement, while recovering from mistakes.

[ep-func]: {{ page.root }}/03-func-R/
[refactoring]: https://en.wikipedia.org/wiki/Code_refactoring
